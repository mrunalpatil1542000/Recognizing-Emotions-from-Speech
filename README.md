 Recognizing-Emotions-from-Speech

Speech is natural way of expressing ourselves. Many researchers today consider speech signal as quick and effective process to interconnect between computer and human there by making it a more challenging component of human computer interaction (HCI). Many techniques have been used to recognize emotions from speech. The proposed novel emotion recognition system uses Multi-Layer Perceptron (MLP) classifier and Mel-Frequency Cepstral Coefficients (MFCC)to classify speech signal for detecting embedded emotions in human voice. The proposed system builds a learning model to analyse speech signal & prediction and define the accuracy of the model.

Recognizes various emotions like happy, anger, disgust, sad and fearful using MFCC (Mel-Frequency Cepstral Coefficient) feature extraction technique from dataset.
Developing using RAVDESS dataset which has 24 actors (Male and Female) which has 1440 files.
Developed using deep learning concept like MLP Classifier and python knowledge.
Tools used to developed the system is Jupyter Notebook in Anaconda 3.

Link for the Dataset to download
   https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio
